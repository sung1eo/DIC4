{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "821340e8",
   "metadata": {},
   "source": [
    "### import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b5ce49-318f-404d-847a-a3abdc5bbe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### default\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "### modeling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "### visualization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb90687",
   "metadata": {},
   "source": [
    "### 파라미터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b232c80-41a8-433a-84d3-21c83e4d7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 센서 종류\n",
    "sensors = ['C7H8', 'TMA', 'VOC', 'CO2', 'HCHO', 'H2S', 'NH3', 'CH3SH', 'SO2', 'NO2', 'CO']\n",
    "cols = ['C7H8', 'TMA', 'VOC', 'CO2', 'HCHO', 'H2S', 'NH3', 'CH3SH', 'SO2', 'NO2', 'CO', 'reg_date', 'label_type']\n",
    "\n",
    "NUM_CLASSES = 6\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc073e9",
   "metadata": {},
   "source": [
    "### 데이터 불러오기, 학습/검증 데이터로 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4605ef-617a-41b7-a1a2-c88a84a84d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('df_type_total_230403.csv') # 3~7차 데이터셋\n",
    "df = pd.read_csv('/home2/datapf/skim/DIC4/s.kim/df_type_total_230407.csv') # 3~9차 데이터셋\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e8fb328",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[pd.to_datetime(df['reg_date']) <= datetime(2023,4,1)]\n",
    "df_test = df[pd.to_datetime(df['reg_date']) > datetime(2023,4,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "676a30c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11627, 6531, 5096)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aadbb74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B     2640\n",
       "ME    2456\n",
       "Name: label_type, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b922d270",
   "metadata": {},
   "source": [
    "### 함수정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edff58fb",
   "metadata": {},
   "source": [
    "#### data processing / plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad5d0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 만들기 위한 실험 구분\n",
    "def create_experiment_number(data):\n",
    "    data['lagged'] = data['reg_date'].shift(-1)\n",
    "    data['reg_date'] = pd.to_datetime(data['reg_date'])\n",
    "    data['lagged'] = pd.to_datetime(data['lagged'])\n",
    "    data['diff'] = data['lagged'] - data['reg_date']\n",
    "    data['diff'] = data['diff'] /  pd.Timedelta('1s')\n",
    "    data['diff'].fillna(1,inplace=True) #마지막줄 채워주기 위함\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    num = 0\n",
    "    for i in range(len(data)):\n",
    "        if data.loc[i,'diff'] in [1,2]:\n",
    "            data.loc[i,'exp_num'] = num\n",
    "        else:\n",
    "            data.loc[i,'exp_num'] = num\n",
    "            num+=1\n",
    "    data.drop(['lagged','diff'], axis='columns', inplace=True)\n",
    "    return data\n",
    "\n",
    "# 실험 별로 시계열 데이터셋을 만들기 위한 함수\n",
    "def create_windows(data, window_size=5):\n",
    "    X = []\n",
    "    y = []\n",
    "    for num, group_df in data.groupby('exp_num'):\n",
    "        data = group_df.iloc[:, 0:11].values\n",
    "        labels = group_df['label_type'].values\n",
    "        for i in range(len(data) - window_size + 1):\n",
    "            X.append(data[i:i+window_size])\n",
    "            y.append(labels[window_size-1])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 데이터셋을 train/valid/test로 나누는 함수\n",
    "def split_data(X, y, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    assert train_ratio + val_ratio + test_ratio == 1, \"The sum of the ratios must be equal to 1.\"\n",
    "    assert len(X) == len(y), \"The length of X and y must be the same.\"\n",
    "    \n",
    "    # numpy 배열을 torch 텐서로 변환합니다.\n",
    "    X = torch.from_numpy(X)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # 데이터를 무작위로 섞기 위한 인덱스를 생성합니다.\n",
    "    shuffled_indices = torch.randperm(len(X))\n",
    "\n",
    "    # 비율에 따라 인덱스를 설정합니다.\n",
    "    train_cnt = int(len(X) * train_ratio)\n",
    "    val_cnt = int(len(X) * val_ratio)\n",
    "    test_cnt = len(X) - (train_cnt + val_cnt)\n",
    "\n",
    "    # 데이터를 train, validation, test 셋으로 나눕니다 (torch.index_select 사용).\n",
    "    X_train, X_val, X_test = torch.index_select(X, dim=0, index=shuffled_indices).split([train_cnt, val_cnt, test_cnt], dim=0)\n",
    "    y_train, y_val, y_test = torch.index_select(y, dim=0, index=shuffled_indices).split([train_cnt, val_cnt, test_cnt], dim=0)\n",
    "    \n",
    "    print('---'*30)\n",
    "    print('Splitting Complete')\n",
    "    print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# torch dataloader를 만들기 위한 함수\n",
    "def create_classification_dataset(data, window_size, batch_size):\n",
    "    X, y = create_windows(data, window_size)\n",
    "        \n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(y)\n",
    "    y = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "\n",
    "    X = X.transpose(0,2,1)\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.float32)\n",
    "       \n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = split_data(X, y)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # DataLoader 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, encoder, scaler\n",
    "\n",
    "# torch dataloader를 만들기 위한 함수\n",
    "def create_test_dataset(data, window_size, batch_size, scaler, encoder):\n",
    "    X, y = create_windows(data, window_size)\n",
    "\n",
    "    X = scaler.transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
    "    y = encoder.transform(y)\n",
    "    y = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "\n",
    "    X = X.transpose(0,2,1)\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.float32)\n",
    "\n",
    "    # numpy 배열을 torch 텐서로 변환합니다.\n",
    "    X = torch.from_numpy(X)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    real_test_dataset = TensorDataset(X, y)\n",
    "\n",
    "    # DataLoader 생성\n",
    "    real_test_loader = DataLoader(real_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return real_test_loader\n",
    "\n",
    "def plot_metrics(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    acc_list = [t.item() for t in history['train_acc']]\n",
    "    val_acc_list = [t.item() for t in history['val_acc']]\n",
    "    plt.plot(acc_list)\n",
    "    plt.plot(val_acc_list)\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='best')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='best')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62d7db",
   "metadata": {},
   "source": [
    "#### 학습용 데이터에 대한 증강 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8277609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(data, mean, std_dev, noise_factor=0.1):\n",
    "    noise = np.random.normal(0, std_dev * noise_factor, data.shape)\n",
    "    augmented_data = data + noise\n",
    "    augmented_data = np.where(augmented_data <= 0, data, augmented_data)\n",
    "    return augmented_data\n",
    "\n",
    "def scale_data_uniform_distribution(data, min_scale=0.8, max_scale=1.2):\n",
    "    scaling_factor = np.random.uniform(min_scale, max_scale)\n",
    "    augmented_data = data * scaling_factor\n",
    "    # 방어 로직 추가\n",
    "    augmented_data = np.where(augmented_data <= 0, data, augmented_data)\n",
    "    return augmented_data\n",
    "\n",
    "def scale_data_normal_distribution(data, mean_scale=1.0, std_dev_scale=0.1):\n",
    "    scaling_factor = np.random.normal(mean_scale, std_dev_scale)\n",
    "    augmented_data = data * scaling_factor\n",
    "    # 방어 로직 추가\n",
    "    augmented_data = np.where(augmented_data <= 0, data, augmented_data)\n",
    "    return augmented_data\n",
    "\n",
    "def shift_data(data, min_shift=-0.5, max_shift=0.5):\n",
    "    shift_value = np.random.uniform(min_shift, max_shift)\n",
    "    augmented_data = data + shift_value\n",
    "    # 방어 로직 추가\n",
    "    augmented_data = np.where(augmented_data <= 0, data, augmented_data)\n",
    "    return augmented_data\n",
    "\n",
    "def apply_jitter(data, mean, jitter_factor=0.05):\n",
    "    jitter = np.random.uniform(-mean*jitter_factor, mean*jitter_factor, data.shape)\n",
    "    augmented_data = data + jitter\n",
    "    # 방어 로직 추가\n",
    "    augmented_data = np.where(augmented_data <= 0, data, augmented_data)\n",
    "    return augmented_data\n",
    "\n",
    "def magnitude_warping_cubic_spline(data, num_knots=4, warping_range=0.2):\n",
    "    knot_xs = np.linspace(0, 1, num_knots)\n",
    "    # knot_ys = np.random.uniform(1 - warping_range, 1 + warping_range, num_knots)\n",
    "    knot_ys = np.random.normal(1.0, warping_range, num_knots)\n",
    "    cubic_spline = CubicSpline(knot_xs, knot_ys)\n",
    "    \n",
    "    data_len = len(data)\n",
    "    xs = np.linspace(0, 1, data_len)\n",
    "    warp_curve = cubic_spline(xs)\n",
    "    \n",
    "    # Reshape the warp_curve to match the data dimensions\n",
    "    warp_curve = warp_curve.reshape(-1, 1)\n",
    "    \n",
    "    warped_data = data * warp_curve\n",
    "    warped_data = np.where(warped_data <= 0, data, warped_data)\n",
    "    return warped_data\n",
    "\n",
    "def time_warping_cubic_spline(data, num_knots=4, warping_range=0.2):\n",
    "    knot_xs = np.linspace(0, 1, num_knots)\n",
    "    # knot_ys = np.random.uniform(1 - warping_range, 1 + warping_range, num_knots)\n",
    "    knot_ys = np.random.normal(1.0, warping_range, num_knots)\n",
    "    cubic_spline = CubicSpline(knot_xs, knot_ys)\n",
    "    \n",
    "    data_len = len(data)\n",
    "    xs = np.linspace(0, 1, data_len)\n",
    "    warp_curve = cubic_spline(xs)\n",
    "    \n",
    "    warped_xs = np.cumsum(warp_curve) / np.sum(warp_curve)\n",
    "    \n",
    "    # Use a loop to process each column of the data array independently\n",
    "    warped_data = np.zeros_like(data)\n",
    "    for i in range(data.shape[1]):\n",
    "        warped_data[:, i] = np.interp(xs, warped_xs, data[:, i])\n",
    "    \n",
    "    # 방어 로직 추가\n",
    "    warped_data = np.where(warped_data <= 0, data, warped_data)\n",
    "    return warped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f352df05",
   "metadata": {},
   "source": [
    "### 데이터 증강"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcf9057",
   "metadata": {},
   "source": [
    "#### 데이터 전처리(실험 구분)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2af697f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C7H8</th>\n",
       "      <th>TMA</th>\n",
       "      <th>VOC</th>\n",
       "      <th>CO2</th>\n",
       "      <th>HCHO</th>\n",
       "      <th>H2S</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CH3SH</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>reg_date</th>\n",
       "      <th>label_type</th>\n",
       "      <th>exp_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.192282</td>\n",
       "      <td>435.1554</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.303789</td>\n",
       "      <td>0.809661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050841</td>\n",
       "      <td>0.012979</td>\n",
       "      <td>0.323783</td>\n",
       "      <td>2023-03-13 12:52:05</td>\n",
       "      <td>B</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.357748</td>\n",
       "      <td>431.6093</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.528598</td>\n",
       "      <td>3.488177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291240</td>\n",
       "      <td>2023-03-13 12:52:06</td>\n",
       "      <td>B</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.498292</td>\n",
       "      <td>326.9812</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.688187</td>\n",
       "      <td>9.506214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230189</td>\n",
       "      <td>2023-03-13 12:52:08</td>\n",
       "      <td>B</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.728273</td>\n",
       "      <td>295.0207</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.815260</td>\n",
       "      <td>9.946881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205769</td>\n",
       "      <td>2023-03-13 12:52:09</td>\n",
       "      <td>B</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002350</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>1.036101</td>\n",
       "      <td>324.5227</td>\n",
       "      <td>0.79195</td>\n",
       "      <td>0.911156</td>\n",
       "      <td>9.946881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150823</td>\n",
       "      <td>2023-03-13 12:52:10</td>\n",
       "      <td>B</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       C7H8       TMA       VOC       CO2     HCHO       H2S       NH3  CH3SH  \\\n",
       "0  0.000000  0.000026  0.192282  435.1554  0.00000  0.303789  0.809661    0.0   \n",
       "1  0.000000  0.000034  0.357748  431.6093  0.00000  0.528598  3.488177    0.0   \n",
       "2  0.000000  0.000045  0.498292  326.9812  0.00000  0.688187  9.506214    0.0   \n",
       "3  0.000944  0.000057  0.728273  295.0207  0.00000  0.815260  9.946881    0.0   \n",
       "4  0.002350  0.000073  1.036101  324.5227  0.79195  0.911156  9.946881    0.0   \n",
       "\n",
       "        SO2       NO2        CO            reg_date label_type  exp_num  \n",
       "0  0.050841  0.012979  0.323783 2023-03-13 12:52:05          B      0.0  \n",
       "1  0.190711  0.000000  0.291240 2023-03-13 12:52:06          B      0.0  \n",
       "2  0.233748  0.000000  0.230189 2023-03-13 12:52:08          B      0.0  \n",
       "3  0.176365  0.000000  0.205769 2023-03-13 12:52:09          B      0.0  \n",
       "4  0.130409  0.000000  0.150823 2023-03-13 12:52:10          B      0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = create_experiment_number(df_train)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0dc6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each sensor (column) for each class\n",
    "class_stats = {}\n",
    "\n",
    "for label in df_train['label_type'].unique():\n",
    "    class_data = df_train[df_train['label_type']==label]\n",
    "    class_data = class_data[sensors].values\n",
    "    means = np.mean(class_data, axis=0)\n",
    "    std_devs = np.std(class_data, axis=0)\n",
    "    class_stats[label] = {'means': means, 'std_devs': std_devs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbf564f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentation methods\n",
    "raw_data = df_train[sensors].values\n",
    "labels = df_train['label_type'].values\n",
    "reg_dates = df_train['reg_date'].values\n",
    "exp_nums = df_train['exp_num'].values\n",
    "\n",
    "augmented_data_1 = []\n",
    "for row, label in zip(raw_data, labels):\n",
    "    augmented_row = np.zeros(row.shape)\n",
    "    for i, value in enumerate(row):\n",
    "        aug_value = add_gaussian_noise(value, class_stats[label]['means'][i], class_stats[label]['std_devs'][i], noise_factor=0.1)\n",
    "        augmented_row[i] = aug_value\n",
    "    augmented_data_1.append(augmented_row)\n",
    "augmented_data_1 = np.array(augmented_data_1)\n",
    "\n",
    "augmented_data_2 = []\n",
    "for row, label in zip(raw_data, labels):\n",
    "    augmented_row = np.zeros(row.shape)\n",
    "    for i, value in enumerate(row):\n",
    "        aug_value = scale_data_uniform_distribution(value)\n",
    "        augmented_row[i] = aug_value\n",
    "    augmented_data_2.append(augmented_row)\n",
    "augmented_data_2 = np.array(augmented_data_2)\n",
    "\n",
    "augmented_data_3 = []\n",
    "for row, label in zip(raw_data, labels):\n",
    "    augmented_row = np.zeros(row.shape)\n",
    "    for i, value in enumerate(row):\n",
    "        aug_value = scale_data_normal_distribution(value)\n",
    "        augmented_row[i] = aug_value\n",
    "    augmented_data_3.append(augmented_row)\n",
    "augmented_data_3 = np.array(augmented_data_3)\n",
    "\n",
    "# augmented_data_4 = []\n",
    "# for row, label in zip(raw_data, labels):\n",
    "#     augmented_row = np.zeros(row.shape)\n",
    "#     for i, value in enumerate(row):\n",
    "#         aug_value = shift_data(value)\n",
    "#         augmented_row[i] = aug_value\n",
    "#     augmented_data_4.append(augmented_row)\n",
    "# augmented_data_4 = np.array(augmented_data_4)\n",
    "\n",
    "augmented_data_5 = []\n",
    "for row, label in zip(raw_data, labels):\n",
    "    augmented_row = np.zeros(row.shape)\n",
    "    for i, value in enumerate(row):\n",
    "        aug_value = apply_jitter(value, class_stats[label]['means'][i] )\n",
    "        augmented_row[i] = aug_value\n",
    "    augmented_data_5.append(augmented_row)\n",
    "augmented_data_5 = np.array(augmented_data_5)\n",
    "\n",
    "augmented_data_6 = []\n",
    "augmented_data_7 = []\n",
    "for exp in df_train['exp_num'].unique():\n",
    "    each = df_train[df_train['exp_num']==exp]\n",
    "    each = each[sensors].values\n",
    "    augmented_data_6.append(magnitude_warping_cubic_spline(each))\n",
    "    augmented_data_7.append(time_warping_cubic_spline(each))\n",
    "\n",
    "# convert (120, ) ==> (6531, 11)\n",
    "augmented_data_6 = np.vstack(augmented_data_6) \n",
    "augmented_data_7 = np.vstack(augmented_data_7)\n",
    "\n",
    "augmented_data_c1 = []\n",
    "for row, label in zip(raw_data, labels):\n",
    "    augmented_row = np.zeros(row.shape)\n",
    "    for i, value in enumerate(row):\n",
    "        aug_value = add_gaussian_noise(value, class_stats[label]['means'][i], class_stats[label]['std_devs'][i], noise_factor=0.1)\n",
    "        aug_value = scale_data_uniform_distribution(aug_value)\n",
    "        aug_value = apply_jitter(aug_value, class_stats[label]['means'][i])\n",
    "        augmented_row[i] = aug_value\n",
    "    augmented_data_c1.append(augmented_row)\n",
    "augmented_data_c1 = np.array(augmented_data_c1)\n",
    "augmented_data_c1 = pd.DataFrame(augmented_data_c1, columns=sensors)\n",
    "augmented_data_c1['exp_num'] = exp_nums\n",
    "\n",
    "augmented_data_8 = []\n",
    "augmented_data_9 = []\n",
    "for exp in augmented_data_c1['exp_num'].unique():\n",
    "    each = augmented_data_c1[augmented_data_c1['exp_num']==exp]\n",
    "    each = each[sensors].values\n",
    "    augmented_data_8.append(magnitude_warping_cubic_spline(each))\n",
    "    augmented_data_9.append(time_warping_cubic_spline(each))\n",
    "\n",
    "# convert (120, ) ==> (6531, 11)\n",
    "augmented_data_8 = np.vstack(augmented_data_8) \n",
    "augmented_data_9 = np.vstack(augmented_data_9)\n",
    "\n",
    "# Save augmented data to a new DataFrame\n",
    "augmented_df_1 = pd.DataFrame(augmented_data_1, columns=sensors)\n",
    "augmented_df_1['label_type'] = labels\n",
    "augmented_df_1['reg_date'] = reg_dates\n",
    "augmented_df_1['exp_num'] = exp_nums\n",
    "augmented_df_1['aug_type'] = 'gaussian_noise'\n",
    "augmented_df_2 = pd.DataFrame(augmented_data_2, columns=sensors)\n",
    "augmented_df_2['label_type'] = labels\n",
    "augmented_df_2['reg_date'] = reg_dates\n",
    "augmented_df_2['exp_num'] = exp_nums\n",
    "augmented_df_2['aug_type'] = 'scaling(uniform)'\n",
    "augmented_df_3 = pd.DataFrame(augmented_data_3, columns=sensors)\n",
    "augmented_df_3['label_type'] = labels\n",
    "augmented_df_3['reg_date'] = reg_dates\n",
    "augmented_df_3['exp_num'] = exp_nums\n",
    "augmented_df_3['aug_type'] = 'scaling(normal)'\n",
    "# augmented_df_4 = pd.DataFrame(augmented_data_4, columns=sensors)\n",
    "# augmented_df_4['label_type'] = labels\n",
    "# augmented_df_4['reg_date'] = reg_dates\n",
    "# augmented_df_4['exp_num'] = exp_nums\n",
    "augmented_df_5 = pd.DataFrame(augmented_data_5, columns=sensors)\n",
    "augmented_df_5['label_type'] = labels\n",
    "augmented_df_5['reg_date'] = reg_dates\n",
    "augmented_df_5['exp_num'] = exp_nums\n",
    "augmented_df_5['aug_type'] = 'jittering'\n",
    "augmented_df_6 = pd.DataFrame(augmented_data_6, columns=sensors)\n",
    "augmented_df_6['label_type'] = labels\n",
    "augmented_df_6['reg_date'] = reg_dates\n",
    "augmented_df_6['exp_num'] = exp_nums\n",
    "augmented_df_6['aug_type'] = 'magnitude_warping'\n",
    "augmented_df_7 = pd.DataFrame(augmented_data_7, columns=sensors)\n",
    "augmented_df_7['label_type'] = labels\n",
    "augmented_df_7['reg_date'] = reg_dates\n",
    "augmented_df_7['exp_num'] = exp_nums\n",
    "augmented_df_7['aug_type'] = 'time_warping'\n",
    "augmented_df_8 = pd.DataFrame(augmented_data_8, columns=sensors)\n",
    "augmented_df_8['label_type'] = labels\n",
    "augmented_df_8['reg_date'] = reg_dates\n",
    "augmented_df_8['exp_num'] = exp_nums\n",
    "augmented_df_8['aug_type'] = 'combination1'\n",
    "augmented_df_9 = pd.DataFrame(augmented_data_9, columns=sensors)\n",
    "augmented_df_9['label_type'] = labels\n",
    "augmented_df_9['reg_date'] = reg_dates\n",
    "augmented_df_9['exp_num'] = exp_nums\n",
    "augmented_df_9['aug_type'] = 'combination2'\n",
    "\n",
    "augmented_df = pd.concat([augmented_df_1, augmented_df_2, augmented_df_3\n",
    "# , augmented_df_4\n",
    ", augmented_df_5, augmented_df_6, augmented_df_7, augmented_df_8, augmented_df_9])\n",
    "augmented_df.reset_index(drop=True, inplace=True)\n",
    "augmented_df['is_aug'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36cfaa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['is_aug'] = 0\n",
    "df_train['aug_type'] = 'original'\n",
    "df_train_total = pd.concat([df_train,augmented_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e1519b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    52248\n",
       "0     6531\n",
       "Name: is_aug, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_total['is_aug'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09cdb7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>C7H8</th>\n",
       "      <th>TMA</th>\n",
       "      <th>VOC</th>\n",
       "      <th>CO2</th>\n",
       "      <th>HCHO</th>\n",
       "      <th>H2S</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CH3SH</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>exp_num</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_aug</th>\n",
       "      <th>label_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">0</th>\n",
       "      <th>A</th>\n",
       "      <td>0.009780</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.315670</td>\n",
       "      <td>3282.466437</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.888562</td>\n",
       "      <td>2.129120</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>0.024296</td>\n",
       "      <td>0.011137</td>\n",
       "      <td>0.574519</td>\n",
       "      <td>76.753223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.062161</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>3.786709</td>\n",
       "      <td>280.845259</td>\n",
       "      <td>9.511448</td>\n",
       "      <td>0.894038</td>\n",
       "      <td>9.492534</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.007592</td>\n",
       "      <td>0.012404</td>\n",
       "      <td>0.588091</td>\n",
       "      <td>56.766493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.003430</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.275784</td>\n",
       "      <td>2549.303650</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.939731</td>\n",
       "      <td>2.102958</td>\n",
       "      <td>0.010678</td>\n",
       "      <td>0.035658</td>\n",
       "      <td>0.006109</td>\n",
       "      <td>0.274356</td>\n",
       "      <td>77.087737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EN</th>\n",
       "      <td>0.026686</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.482237</td>\n",
       "      <td>39.275725</td>\n",
       "      <td>0.007571</td>\n",
       "      <td>0.805855</td>\n",
       "      <td>4.409067</td>\n",
       "      <td>0.002131</td>\n",
       "      <td>0.014499</td>\n",
       "      <td>0.023759</td>\n",
       "      <td>0.398367</td>\n",
       "      <td>66.741490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ME</th>\n",
       "      <td>0.036099</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.794137</td>\n",
       "      <td>192.520571</td>\n",
       "      <td>0.775849</td>\n",
       "      <td>1.014770</td>\n",
       "      <td>6.617821</td>\n",
       "      <td>0.073342</td>\n",
       "      <td>0.006931</td>\n",
       "      <td>0.007405</td>\n",
       "      <td>0.696040</td>\n",
       "      <td>56.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MO</th>\n",
       "      <td>0.016899</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.768089</td>\n",
       "      <td>78.141789</td>\n",
       "      <td>0.006571</td>\n",
       "      <td>0.811076</td>\n",
       "      <td>8.925683</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>0.010757</td>\n",
       "      <td>0.021719</td>\n",
       "      <td>0.542908</td>\n",
       "      <td>62.390223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">1</th>\n",
       "      <th>A</th>\n",
       "      <td>0.009763</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.316687</td>\n",
       "      <td>3301.816065</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.887493</td>\n",
       "      <td>2.119665</td>\n",
       "      <td>0.009723</td>\n",
       "      <td>0.024466</td>\n",
       "      <td>0.011981</td>\n",
       "      <td>0.580672</td>\n",
       "      <td>76.753223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.062132</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>3.792596</td>\n",
       "      <td>283.393924</td>\n",
       "      <td>9.507166</td>\n",
       "      <td>0.896619</td>\n",
       "      <td>9.522685</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.007958</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.592828</td>\n",
       "      <td>56.766493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.003458</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.274683</td>\n",
       "      <td>2510.382076</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.928356</td>\n",
       "      <td>2.070617</td>\n",
       "      <td>0.010938</td>\n",
       "      <td>0.035648</td>\n",
       "      <td>0.006714</td>\n",
       "      <td>0.275686</td>\n",
       "      <td>77.087737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EN</th>\n",
       "      <td>0.026626</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.482268</td>\n",
       "      <td>42.043345</td>\n",
       "      <td>0.009970</td>\n",
       "      <td>0.802425</td>\n",
       "      <td>4.392820</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.014741</td>\n",
       "      <td>0.025362</td>\n",
       "      <td>0.403486</td>\n",
       "      <td>66.741490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ME</th>\n",
       "      <td>0.036130</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.790923</td>\n",
       "      <td>195.864418</td>\n",
       "      <td>0.785160</td>\n",
       "      <td>1.011180</td>\n",
       "      <td>6.571229</td>\n",
       "      <td>0.075791</td>\n",
       "      <td>0.007253</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.699077</td>\n",
       "      <td>56.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MO</th>\n",
       "      <td>0.016951</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.766739</td>\n",
       "      <td>80.458056</td>\n",
       "      <td>0.007730</td>\n",
       "      <td>0.809514</td>\n",
       "      <td>8.896214</td>\n",
       "      <td>0.006614</td>\n",
       "      <td>0.011022</td>\n",
       "      <td>0.023582</td>\n",
       "      <td>0.540151</td>\n",
       "      <td>62.390223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       C7H8       TMA       VOC          CO2      HCHO  \\\n",
       "is_aug label_type                                                        \n",
       "0      A           0.009780  0.000033  0.315670  3282.466437  0.000512   \n",
       "       B           0.062161  0.000261  3.786709   280.845259  9.511448   \n",
       "       C           0.003430  0.000031  0.275784  2549.303650  0.000227   \n",
       "       EN          0.026686  0.000040  0.482237    39.275725  0.007571   \n",
       "       ME          0.036099  0.000114  0.794137   192.520571  0.775849   \n",
       "       MO          0.016899  0.000060  0.768089    78.141789  0.006571   \n",
       "1      A           0.009763  0.000033  0.316687  3301.816065  0.000804   \n",
       "       B           0.062132  0.000260  3.792596   283.393924  9.507166   \n",
       "       C           0.003458  0.000030  0.274683  2510.382076  0.000349   \n",
       "       EN          0.026626  0.000040  0.482268    42.043345  0.009970   \n",
       "       ME          0.036130  0.000114  0.790923   195.864418  0.785160   \n",
       "       MO          0.016951  0.000060  0.766739    80.458056  0.007730   \n",
       "\n",
       "                        H2S       NH3     CH3SH       SO2       NO2        CO  \\\n",
       "is_aug label_type                                                               \n",
       "0      A           0.888562  2.129120  0.009476  0.024296  0.011137  0.574519   \n",
       "       B           0.894038  9.492534  0.000244  0.007592  0.012404  0.588091   \n",
       "       C           0.939731  2.102958  0.010678  0.035658  0.006109  0.274356   \n",
       "       EN          0.805855  4.409067  0.002131  0.014499  0.023759  0.398367   \n",
       "       ME          1.014770  6.617821  0.073342  0.006931  0.007405  0.696040   \n",
       "       MO          0.811076  8.925683  0.006587  0.010757  0.021719  0.542908   \n",
       "1      A           0.887493  2.119665  0.009723  0.024466  0.011981  0.580672   \n",
       "       B           0.896619  9.522685  0.000290  0.007958  0.013986  0.592828   \n",
       "       C           0.928356  2.070617  0.010938  0.035648  0.006714  0.275686   \n",
       "       EN          0.802425  4.392820  0.002164  0.014741  0.025362  0.403486   \n",
       "       ME          1.011180  6.571229  0.075791  0.007253  0.008258  0.699077   \n",
       "       MO          0.809514  8.896214  0.006614  0.011022  0.023582  0.540151   \n",
       "\n",
       "                     exp_num  \n",
       "is_aug label_type             \n",
       "0      A           76.753223  \n",
       "       B           56.766493  \n",
       "       C           77.087737  \n",
       "       EN          66.741490  \n",
       "       ME          56.117647  \n",
       "       MO          62.390223  \n",
       "1      A           76.753223  \n",
       "       B           56.766493  \n",
       "       C           77.087737  \n",
       "       EN          66.741490  \n",
       "       ME          56.117647  \n",
       "       MO          62.390223  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_total.groupby(['is_aug','label_type']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cbf4de",
   "metadata": {},
   "source": [
    "### pytorch를 사용한 딥러닝 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31a04a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter setting\n",
    "batch_size = 32\n",
    "num_classes = NUM_CLASSES\n",
    "num_epochs = 50\n",
    "window_size = 7\n",
    "feature_len = 11\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Detect if we have a GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d1bbb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62802355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "Splitting Complete\n",
      "torch.Size([46447, 11, 7]) torch.Size([46447, 6]) torch.Size([5805, 11, 7]) torch.Size([5805, 6]) torch.Size([5807, 11, 7]) torch.Size([5807, 6])\n"
     ]
    }
   ],
   "source": [
    "# Dataloader 구축\n",
    "# data shape: (batch_size x input_size x seq_len) => (32, 11, 5)\n",
    "train_loader, valid_loader, test_loader, encoder, scaler = create_classification_dataset(df_train_total, window_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98442f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 - Inputs shape: torch.Size([32, 11, 7]), Targets shape: torch.Size([32, 6])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader에서 데이터 형태 확인\n",
    "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx + 1} - Inputs shape: {inputs.shape}, Targets shape: {targets.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f086e6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 - Inputs shape: torch.Size([32, 11, 7]), Targets shape: torch.Size([32, 6])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader에서 데이터 형태 확인\n",
    "for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "    print(f\"Batch {batch_idx + 1} - Inputs shape: {inputs.shape}, Targets shape: {targets.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4f7214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-dimensional convolution layer로 구성된 CNN 모델\n",
    "# 2개의 1-dimensional convolution layer와 1개의 fully-connected layer로 구성되어 있음\n",
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, num_classes, dropout_rate=0.2):\n",
    "\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        super(CNN_1D, self).__init__()\n",
    "        # 첫 번째 1-dimensional convolution layer 구축\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channel, out_channel, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "#             nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            # nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        # 두 번째 1-dimensional convolution layer 구축\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(out_channel, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "#             nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "            # nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        # fully-connected layer 구축\n",
    "        self.fc = nn.Linear(32*1, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fae5962d-e166-4351-b0ab-4ae336ea7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_rate=0.2, bidirectional=False, rnn_type='rnn'):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_type = rnn_type.lower()\n",
    "        self.num_directions = 2 if bidirectional == True else 1\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        if self.rnn_type == 'rnn':\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate, bidirectional=bidirectional)\n",
    "        elif self.rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate, bidirectional=bidirectional)\n",
    "        elif self.rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate, bidirectional=bidirectional)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid rnn_type: {rnn_type}, expected one of ['rnn', 'lstm', 'gru']\")\n",
    "        \n",
    "        # bidirectional에 따른 fc layer 구축\n",
    "        # bidirectional 여부에 따라 hidden state의 shape가 달라짐 (True: 2 * hidden_size, False: hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(self.num_directions * hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # data dimension: (batch_size x input_size x seq_len) -> (batch_size x seq_len x input_size)로 변환\n",
    "        x = torch.transpose(x, 1, 2) # x.permute(0, 2, 1) 와 결과적으로는 동일\n",
    "        rnn_out, hidden = self.rnn(x)\n",
    "\n",
    "        # LSTMs, the hidden state is a tuple containing both the hidden state and cell state. \n",
    "        if self.rnn_type == 'lstm':\n",
    "            hidden_state, _ = hidden\n",
    "        else:\n",
    "            hidden_state = hidden\n",
    "\n",
    "        if bidirectional:\n",
    "            forward_out = rnn_out[:, -1, :self.hidden_size]\n",
    "            backward_out = rnn_out[:, 0, self.hidden_size:]\n",
    "            last_time_step_out = torch.cat((forward_out, backward_out), dim=1)\n",
    "        else:\n",
    "            last_time_step_out = rnn_out[:, -1, :]\n",
    "        \n",
    "        dropped_out = self.dropout(last_time_step_out)\n",
    "        out = self.fc(dropped_out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d630dc08-a12a-4ad9-a14e-456a11dbc609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.2, kernel_size=3, num_layers=1):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=32, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.dropout_cnn_rnn = nn.Dropout(dropout_rate)\n",
    "        self.rnn = nn.GRU(input_size=256, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout_rate))\n",
    "        self.dropout_rnn_fc = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, window_size, feature_size)\n",
    "        x = self.dropout_cnn_rnn(x)\n",
    "        output, hidden = self.rnn(x)\n",
    "        output = self.dropout_rnn_fc(output[:, -1, :])\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cf7183d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_1D(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv1d(11, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=32, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 1D CNN 구축\n",
    "cnn_default = CNN_1D(in_channel=feature_len, out_channel=16, num_classes=NUM_CLASSES, dropout_rate=0) # in_channel은 feature의 갯수\n",
    "cnn_default = cnn_default.to(device)\n",
    "print(cnn_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b47af318-6a68-4929-bdda-1d3e46b01f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_1D(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv1d(11, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=32, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 1D CNN with dropout 구축\n",
    "cnn_dropout = CNN_1D(in_channel=feature_len, out_channel=16, num_classes=NUM_CLASSES, dropout_rate=0.25) # in_channel은 feature의 갯수\n",
    "cnn_dropout = cnn_dropout.to(device)\n",
    "print(cnn_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf04a14-0714-480d-959c-49971d8fef69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3738ab95-732d-4dc2-8511-22a2f1cbc947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dd511a-be04-4cde-bd73-b97469fcba80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c1161-e4d4-4d84-a88a-57f3cb788337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2a93f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, num_epochs, optimizer):\n",
    "    since = time.time()\n",
    "\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        train_corrects = 0\n",
    "        val_corrects = 0\n",
    "        train_total = 0\n",
    "        val_total = 0\n",
    "\n",
    "        # 각 epoch마다 순서대로 training과 validation을 진행\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 모델을 training mode로 설정\n",
    "            else:\n",
    "                model.eval()   # 모델을 validation mode로 설정            \n",
    "\n",
    "            # training과 validation 단계에 맞는 dataloader에 대하여 학습/검증 진행\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # parameter gradients를 0으로 설정\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # training 단계에서만 gradient 업데이트 수행\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # input을 model에 넣어 output을 도출한 후, loss를 계산함\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # output 중 최댓값의 위치에 해당하는 class로 예측을 수행\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward (optimize): training 단계에서만 수행\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # batch별 loss를 축적함\n",
    "                if phase == 'train':\n",
    "                    # running_loss += loss.item() * inputs.size(0)\n",
    "                    # running_corrects += torch.sum(preds == torch.argmax(labels, dim=-1))\n",
    "                    # running_total += labels.size(0)\n",
    "\n",
    "                    train_loss += loss.item() * inputs.size(0)\n",
    "                    # train_loss += loss.item() \n",
    "                    train_corrects += torch.sum(preds == torch.argmax(labels, dim=-1))\n",
    "                    train_total += labels.size(0)\n",
    "\n",
    "                else:\n",
    "                    val_loss += loss.item() * inputs.size(0)\n",
    "                    # val_loss += loss.item()\n",
    "                    val_corrects += torch.sum(preds == torch.argmax(labels, dim=-1))\n",
    "                    val_total += labels.size(0)\n",
    "            \n",
    "        train_loss_epoch = train_loss/train_total\n",
    "        val_loss_epoch = val_loss/val_total\n",
    "        train_acc_epoch = train_corrects.double() /train_total\n",
    "        val_acc_epoch = val_corrects.double() /val_total\n",
    "\n",
    "        # epoch의 loss 및 accuracy 도출\n",
    "        # epoch_loss = running_loss / running_total\n",
    "        # epoch_acc = running_corrects.double() / running_total\n",
    "\n",
    "        # validation 단계에서 validation loss가 감소할 때마다 best model 가중치를 업데이트함\n",
    "        if phase == 'val' and val_acc_epoch > best_acc:\n",
    "            best_acc = val_acc_epoch\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            print('update model parameter')\n",
    "\n",
    "        train_acc_history.append(train_acc_epoch)\n",
    "        val_acc_history.append(val_acc_epoch)\n",
    "        train_loss_history.append(train_loss_epoch)\n",
    "        val_loss_history.append(val_loss_epoch)\n",
    "            \n",
    "            # if phase == 'val':\n",
    "            #     val_acc_history.append(val_acc_epoch)\n",
    "            \n",
    "        print(f'{epoch+1} epoch | Train loss: {train_loss_epoch:.3f}, Valid loss: {val_loss_epoch:.3f}, Train acc: {train_acc_epoch:.3f}, Valid acc: {val_acc_epoch:.3f}')\n",
    "        # print(f'{epoch+1} epoch | Train acc: {train_acc_epoch:.3f}, Valid acc: {val_acc_epoch:.3f}')\n",
    "        print()\n",
    "\n",
    "    # 전체 학습 시간 계산\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # validation loss가 가장 낮았을 때의 best model 가중치를 불러와 best model을 구축함\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    history = {'train_acc':train_acc_history, 'val_acc':val_acc_history, 'train_loss':train_loss_history, 'val_loss':val_loss_history}\n",
    "    \n",
    "    # best model 가중치 저장\n",
    "    # torch.save(best_model_wts, '../output/best_model.pt')\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1418cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trining 단계에서 사용할 Dataloader dictionary 생성\n",
    "dataloaders_dict = {\n",
    "    'train': train_loader,\n",
    "    'val': valid_loader\n",
    "}\n",
    "# loss function 설정\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036140d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "----------\n",
      "update model parameter\n",
      "1 epoch | Train loss: 0.665, Valid loss: 0.432, Train acc: 0.728, Valid acc: 0.836\n",
      "\n",
      "Epoch 2/50\n",
      "----------\n",
      "update model parameter\n",
      "2 epoch | Train loss: 0.372, Valid loss: 0.325, Train acc: 0.853, Valid acc: 0.869\n",
      "\n",
      "Epoch 3/50\n",
      "----------\n",
      "update model parameter\n",
      "3 epoch | Train loss: 0.292, Valid loss: 0.269, Train acc: 0.884, Valid acc: 0.903\n",
      "\n",
      "Epoch 4/50\n",
      "----------\n",
      "4 epoch | Train loss: 0.245, Valid loss: 0.248, Train acc: 0.902, Valid acc: 0.892\n",
      "\n",
      "Epoch 5/50\n",
      "----------\n",
      "update model parameter\n",
      "5 epoch | Train loss: 0.216, Valid loss: 0.209, Train acc: 0.915, Valid acc: 0.922\n",
      "\n",
      "Epoch 6/50\n",
      "----------\n",
      "update model parameter\n",
      "6 epoch | Train loss: 0.196, Valid loss: 0.196, Train acc: 0.923, Valid acc: 0.923\n",
      "\n",
      "Epoch 7/50\n",
      "----------\n",
      "7 epoch | Train loss: 0.179, Valid loss: 0.206, Train acc: 0.930, Valid acc: 0.902\n",
      "\n",
      "Epoch 8/50\n",
      "----------\n",
      "update model parameter\n",
      "8 epoch | Train loss: 0.165, Valid loss: 0.161, Train acc: 0.936, Valid acc: 0.939\n",
      "\n",
      "Epoch 9/50\n",
      "----------\n",
      "update model parameter\n",
      "9 epoch | Train loss: 0.154, Valid loss: 0.158, Train acc: 0.941, Valid acc: 0.939\n",
      "\n",
      "Epoch 10/50\n",
      "----------\n",
      "update model parameter\n",
      "10 epoch | Train loss: 0.145, Valid loss: 0.143, Train acc: 0.944, Valid acc: 0.944\n",
      "\n",
      "Epoch 11/50\n",
      "----------\n",
      "update model parameter\n",
      "11 epoch | Train loss: 0.136, Valid loss: 0.136, Train acc: 0.948, Valid acc: 0.948\n",
      "\n",
      "Epoch 12/50\n",
      "----------\n",
      "update model parameter\n",
      "12 epoch | Train loss: 0.127, Valid loss: 0.129, Train acc: 0.952, Valid acc: 0.951\n",
      "\n",
      "Epoch 13/50\n",
      "----------\n",
      "update model parameter\n",
      "13 epoch | Train loss: 0.120, Valid loss: 0.126, Train acc: 0.955, Valid acc: 0.955\n",
      "\n",
      "Epoch 14/50\n",
      "----------\n",
      "14 epoch | Train loss: 0.114, Valid loss: 0.118, Train acc: 0.957, Valid acc: 0.953\n",
      "\n",
      "Epoch 15/50\n",
      "----------\n",
      "15 epoch | Train loss: 0.108, Valid loss: 0.124, Train acc: 0.959, Valid acc: 0.946\n",
      "\n",
      "Epoch 16/50\n",
      "----------\n",
      "update model parameter\n",
      "16 epoch | Train loss: 0.102, Valid loss: 0.103, Train acc: 0.962, Valid acc: 0.964\n",
      "\n",
      "Epoch 17/50\n",
      "----------\n",
      "17 epoch | Train loss: 0.097, Valid loss: 0.099, Train acc: 0.964, Valid acc: 0.963\n",
      "\n",
      "Epoch 18/50\n",
      "----------\n",
      "18 epoch | Train loss: 0.094, Valid loss: 0.092, Train acc: 0.965, Valid acc: 0.962\n",
      "\n",
      "Epoch 19/50\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "cnn_default_model, history = train_model(cnn_default, dataloaders_dict, criterion, num_epochs, optimizer=optim.Adam(cnn_default.parameters(), lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6472b776-bf74-4d72-a62d-e5c001e1e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "cnn_dropout_model, cnn_dropout_history = train_model(cnn_dropout, dataloaders_dict, criterion, num_epochs, optimizer=optim.Adam(cnn_dropout.parameters(), lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fe1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()   # 모델을 validation mode로 설정\n",
    "\n",
    "    pred_list = []\n",
    "    true_list = []\n",
    "\n",
    "    # test_loader에 대하여 검증 진행 (gradient update 방지)\n",
    "    with torch.no_grad():\n",
    "        corrects = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward\n",
    "            # input을 model에 넣어 output을 도출\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # output 중 최댓값의 위치에 해당하는 class로 예측을 수행\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # batch별 정답 개수를 축적함\n",
    "            corrects += torch.sum(preds == torch.argmax(labels, dim=-1))\n",
    "            total += labels.size(0)\n",
    "\n",
    "            pred_list.extend(preds.detach().numpy())\n",
    "            true_list.extend(torch.argmax(labels, dim=-1).detach().numpy())\n",
    "\n",
    "    # accuracy를 도출함\n",
    "    test_acc = corrects.double() / total\n",
    "    print('Testing Acc: {:.4f}'.format(test_acc))\n",
    "\n",
    "    return pred_list, true_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed358544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 검증 \n",
    "pred_list, true_list = test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a5047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가지표 계산\n",
    "test_acc = accuracy_score(true_list, pred_list)\n",
    "test_rec = recall_score(true_list, pred_list, average='macro')\n",
    "test_prec = precision_score(true_list, pred_list, average='macro')\n",
    "test_f1 = f1_score(true_list, pred_list, average='macro')\n",
    "\n",
    "print('Test Accuracy   : {:.3f}'.format(test_acc))\n",
    "print('Test Sensitivity: {:.3f}'.format(test_rec))\n",
    "print('Test Precision  : {:.3f}'.format(test_prec))\n",
    "print('Test F1 Score   : {:.3f}'.format(test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8734df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix 계산\n",
    "cm = confusion_matrix(y_true=true_list, y_pred=pred_list)\n",
    "cm_df = pd.DataFrame(cm)\n",
    "\n",
    "cm_df.index = encoder.classes_\n",
    "cm_df.columns = encoder.classes_\n",
    "\n",
    "# confusion matrix 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(data=cm_df, annot=True, fmt='d', annot_kws={'size': 15}, cmap='Blues')\n",
    "plt.xlabel('Predicted', size=20)\n",
    "plt.ylabel('True', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b43bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = create_experiment_number(df_test)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dd68fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataloader 구축\n",
    "# data shape: (batch_size x input_size x seq_len) => (32, 11, 5)\n",
    "real_test_dataloader = create_test_dataset(df_test, window_size, batch_size, encoder=encoder, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b885a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_pred_list, r_true_list = test_model(model, real_test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fb62fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_numbers = list(range(0,len(encoder.classes_)))\n",
    "label_dict = dict(zip(encoding_numbers, encoder.inverse_transform(encoding_numbers)))\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da65e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix 계산\n",
    "cm = confusion_matrix(y_true=r_true_list, y_pred=r_pred_list)\n",
    "cm_df = pd.DataFrame(cm, columns=np.unique(r_pred_list), index=np.unique(r_pred_list))\n",
    "\n",
    "new_index = [label_dict[i] for i in cm_df.index]\n",
    "new_columns = [label_dict[i] for i in cm_df.columns]\n",
    "\n",
    "# 인덱스와 컬럼 이름 변경\n",
    "cm_df = cm_df.rename(index=dict(zip(cm_df.index, new_index)), columns=dict(zip(cm_df.columns, new_columns)))\n",
    "\n",
    "# confusion matrix 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(data=cm_df, annot=True, fmt='d', annot_kws={'size': 15}, cmap='Blues')\n",
    "plt.xlabel('Predicted', size=20)\n",
    "plt.ylabel('True', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c58da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4073845fb37e5e91a7e4a5aa5d982e60b118e70ad895c083f27423fb55749714"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
